{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02 - Vector Retrieval mit FAISS\n",
    "\n",
    "Implementierung des FAISS-basierten Vektor-Retrievals für das RAG-Benchmarking.\n",
    "\n",
    "## Ziele:\n",
    "- FAQ-Korpus in Vektoren umwandeln\n",
    "- FAISS-Index aufbauen\n",
    "- Retrieval-Funktion implementieren\n",
    "- Erste Tests mit Testfragen"
   ],
   "id": "c28b86e28242aaa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:51:54.071428Z",
     "start_time": "2025-08-10T17:51:29.680044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer # add to poetry\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n"
   ],
   "id": "2ca41618467ae595",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Daten laden"
   ],
   "id": "6df5fc2e35ffa80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:51:54.255969Z",
     "start_time": "2025-08-10T17:51:54.248145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FAQ-Korpus laden\n",
    "with open('../data/faq_korpus.json', 'r', encoding='utf-8') as f:\n",
    "    faq_documents = json.load(f)\n",
    "\n",
    "# Testfragen laden\n",
    "test_questions = pd.read_csv('../data/fragenliste.csv')\n",
    "\n",
    "print(f\"{len(faq_documents)} FAQ-Dokumente geladen\")\n",
    "print(f\"{len(test_questions)} Testfragen geladen\")"
   ],
   "id": "396885c3be86644a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 FAQ-Dokumente geladen\n",
      "12 Testfragen geladen\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding-Modell initialisieren\n",
    "\n",
    "Wir nutzen ein deutsches/multilinguales Sentence-Transformer-Modell für bessere Performance bei deutschen Texten."
   ],
   "id": "4ceba5f63f7c45e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:39.649815Z",
     "start_time": "2025-08-10T17:51:54.287173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedding-Modell laden (multilingual für deutsche Texte)\n",
    "model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"Embedding-Modell geladen: {model_name}\")\n",
    "print(f\"Embedding-Dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ],
   "id": "b25c697b17a5e742",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25014aff4e0646f4882f71d72ca311f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\PycharmProjects\\rag-benchmark\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lukas\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a732201570a45e7b20c97941498e0b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a93e37e03bf045aca32164ef2f093aea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94e535155d4e4e16800298e4d1c250fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99d47e24e8b641459cd7816aedea9e47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dea5e177870403db1b0a445b59ecb8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9753a2e307e748e58ce4a34351a3147c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2db6ce4489d4b46a4715e19f5ecfdf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d9a58de53a341d09544133779556ef2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d40e588c18834109bdd6654612772964"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Modell geladen: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Embedding-Dimension: 384\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dokumente für Embedding vorbereiten\n",
    "\n",
    "Wir kombinieren Frage und Antwort für bessere Retrieval-Performance."
   ],
   "id": "e3cc5a0ca707afdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:39.669816Z",
     "start_time": "2025-08-10T17:52:39.665521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Texte für Embedding vorbereiten (Frage + Antwort kombiniert)\n",
    "documents_for_embedding = []\n",
    "document_metadata = []\n",
    "\n",
    "for doc in faq_documents:\n",
    "    # Kombiniere Frage und Antwort für besseres Retrieval\n",
    "    combined_text = f\"Frage: {doc['question']}\\n\\nAntwort: {doc['answer']}\"\n",
    "    documents_for_embedding.append(combined_text)\n",
    "\n",
    "    # Metadata für spätere Zuordnung\n",
    "    document_metadata.append({\n",
    "        'id': doc['id'],\n",
    "        'question': doc['question'],\n",
    "        'answer': doc['answer'],\n",
    "        'category': doc['category'],\n",
    "        'keywords': doc['keywords']\n",
    "    })\n",
    "\n",
    "print(f\"{len(documents_for_embedding)} Dokumente für Embedding vorbereitet\")\n",
    "print(f\"\\nBeispiel kombinierter Text:\")\n",
    "print(documents_for_embedding[0][:200] + \"...\")"
   ],
   "id": "d69490dbea93df47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Dokumente für Embedding vorbereitet\n",
      "\n",
      "Beispiel kombinierter Text:\n",
      "Frage: Was ist Retrieval-Augmented Generation (RAG)?\n",
      "\n",
      "Antwort: RAG ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell kann relevante Informationen aus einer ...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embeddings generieren"
   ],
   "id": "63550eeae0ad56b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.008190Z",
     "start_time": "2025-08-10T17:52:39.687423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embeddings für alle Dokumente generieren\n",
    "print(\"Generiere Embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "document_embeddings = embedding_model.encode(\n",
    "    documents_for_embedding,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Embeddings generiert in {end_time - start_time:.2f} Sekunden\")\n",
    "print(f\"Shape: {document_embeddings.shape}\")\n",
    "print(f\"Dimension pro Dokument: {document_embeddings.shape[1]}\")"
   ],
   "id": "c3f5ccadc0010959",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generiere Embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cc8e8a654c5452595eca1a715380851"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generiert in 0.32 Sekunden\n",
      "Shape: (15, 384)\n",
      "Dimension pro Dokument: 384\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## FAISS-Index aufbauen"
   ],
   "id": "f2b1f2063dadabed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.031676Z",
     "start_time": "2025-08-10T17:52:40.024284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FAISS-Index erstellen (Cosinus-Ähnlichkeit)\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product für Cosinus-Ähnlichkeit\n",
    "\n",
    "# Embeddings normalisieren für Cosinus-Ähnlichkeit\n",
    "normalized_embeddings = document_embeddings / np.linalg.norm(document_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Zum Index hinzufügen\n",
    "index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS-Index erstellt\")\n",
    "print(f\"Index-Typ: {type(index)}\")\n",
    "print(f\"Anzahl Vektoren: {index.ntotal}\")\n",
    "print(f\"Dimension: {index.d}\")"
   ],
   "id": "18a20d4d1d7e1c95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS-Index erstellt\n",
      "Index-Typ: <class 'faiss.swigfaiss_avx2.IndexFlatIP'>\n",
      "Anzahl Vektoren: 15\n",
      "Dimension: 384\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieval-Funktion implementieren"
   ],
   "id": "2e94c83eac5d399"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.059411Z",
     "start_time": "2025-08-10T17:52:40.052792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vector_retrieval(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Führt Vektor-Retrieval für eine Anfrage durch\n",
    "\n",
    "    Args:\n",
    "        query (str): Die Suchanfrage\n",
    "        top_k (int): Anzahl der zurückzugebenden Dokumente\n",
    "\n",
    "    Returns:\n",
    "        list: Liste der relevantesten Dokumente mit Scores\n",
    "    \"\"\"\n",
    "    # Query-Embedding generieren\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # Normalisieren für Cosinus-Ähnlichkeit\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "    # Suche im Index\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    # Ergebnisse zusammenstellen\n",
    "    results = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Gültiger Index\n",
    "            result = {\n",
    "                'rank': i + 1,\n",
    "                'score': float(score),\n",
    "                'document_id': document_metadata[idx]['id'],\n",
    "                'question': document_metadata[idx]['question'],\n",
    "                'answer': document_metadata[idx]['answer'],\n",
    "                'category': document_metadata[idx]['category'],\n",
    "                'keywords': document_metadata[idx]['keywords']\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Retrieval-Funktion implementiert\")"
   ],
   "id": "38a70d6329015c2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Funktion implementiert\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Erste Tests mit Beispielabfragen"
   ],
   "id": "c56e3adabea9b040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.124730Z",
     "start_time": "2025-08-10T17:52:40.086530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 1: Einfache Frage zu RAG\n",
    "print(\"=== TEST 1: Einfache RAG-Frage ===\")\n",
    "query1 = \"Was ist RAG?\"\n",
    "results1 = vector_retrieval(query1, top_k=3)\n",
    "\n",
    "print(f\"Query: '{query1}'\")\n",
    "print(\"\\nTop 3 Ergebnisse:\")\n",
    "for result in results1:\n",
    "    print(f\"\\n{result['rank']}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   Frage: {result['question']}\")\n",
    "    print(f\"   Kategorie: {result['category']}\")\n",
    "    print(f\"   Antwort: {result['answer'][:100]}...\")"
   ],
   "id": "6a7650b61939ee57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 1: Einfache RAG-Frage ===\n",
      "Query: 'Was ist RAG?'\n",
      "\n",
      "Top 3 Ergebnisse:\n",
      "\n",
      "1. Score: 0.585\n",
      "   Frage: Was ist Retrieval-Augmented Generation (RAG)?\n",
      "   Kategorie: RAG Basics\n",
      "   Antwort: RAG ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell ka...\n",
      "\n",
      "2. Score: 0.455\n",
      "   Frage: Welche Rolle spielt LangChain in RAG-Systemen?\n",
      "   Kategorie: Tools\n",
      "   Antwort: LangChain bietet einen einheitlichen Framework für RAG-Implementierungen. Es abstrahiert Retriever, ...\n",
      "\n",
      "3. Score: 0.419\n",
      "   Frage: Welche Zukunftstrends gibt es bei RAG-Technologien?\n",
      "   Kategorie: Future Trends\n",
      "   Antwort: Trends umfassen Agentic RAG mit Tool-Usage, Multi-modal RAG für Bilder/Videos, und Adaptive Retrieva...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.165054Z",
     "start_time": "2025-08-10T17:52:40.140665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 2: Technische Frage zu Vektoren\n",
    "print(\"\\n=== TEST 2: Technische Frage ===\")\n",
    "query2 = \"Wie funktioniert Vektorsuche?\"\n",
    "results2 = vector_retrieval(query2, top_k=3)\n",
    "\n",
    "print(f\"Query: '{query2}'\")\n",
    "print(\"\\nTop 3 Ergebnisse:\")\n",
    "for result in results2:\n",
    "    print(f\"\\n{result['rank']}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   Frage: {result['question']}\")\n",
    "    print(f\"   Kategorie: {result['category']}\")\n",
    "    print(f\"   Keywords: {result['keywords']}\")"
   ],
   "id": "6bcd52620b879539",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST 2: Technische Frage ===\n",
      "Query: 'Wie funktioniert Vektorsuche?'\n",
      "\n",
      "Top 3 Ergebnisse:\n",
      "\n",
      "1. Score: 0.793\n",
      "   Frage: Wie funktioniert Vektorsuche in RAG-Systemen?\n",
      "   Kategorie: Vector Retrieval\n",
      "   Keywords: ['vector search', 'embeddings', 'FAISS', 'similarity']\n",
      "\n",
      "2. Score: 0.391\n",
      "   Frage: Was ist Retrieval-Augmented Generation (RAG)?\n",
      "   Kategorie: RAG Basics\n",
      "   Keywords: ['RAG', 'retrieval', 'language model', 'knowledge base']\n",
      "\n",
      "3. Score: 0.381\n",
      "   Frage: Welche Evaluationsmetriken gibt es für RAG-Systeme?\n",
      "   Kategorie: Evaluation\n",
      "   Keywords: ['BLEU', 'ROUGE', 'evaluation', 'metrics', 'GPT-judge']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.204787Z",
     "start_time": "2025-08-10T17:52:40.181309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 3: Vergleichsfrage\n",
    "print(\"\\n=== TEST 3: Vergleichsfrage ===\")\n",
    "query3 = \"Unterschied zwischen Vektor und Graph Retrieval\"\n",
    "results3 = vector_retrieval(query3, top_k=3)\n",
    "\n",
    "print(f\"Query: '{query3}'\")\n",
    "print(\"\\nTop 3 Ergebnisse:\")\n",
    "for result in results3:\n",
    "    print(f\"\\n{result['rank']}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   Frage: {result['question']}\")\n",
    "    print(f\"   Kategorie: {result['category']}\")"
   ],
   "id": "a3bdae32ad60b492",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST 3: Vergleichsfrage ===\n",
      "Query: 'Unterschied zwischen Vektor und Graph Retrieval'\n",
      "\n",
      "Top 3 Ergebnisse:\n",
      "\n",
      "1. Score: 0.725\n",
      "   Frage: Was sind die Vorteile von Graph-basiertem Retrieval?\n",
      "   Kategorie: Graph Retrieval\n",
      "\n",
      "2. Score: 0.586\n",
      "   Frage: Was ist der Unterschied zwischen Dense und Sparse Retrieval?\n",
      "   Kategorie: Retrieval Methods\n",
      "\n",
      "3. Score: 0.567\n",
      "   Frage: Wie funktioniert Vektorsuche in RAG-Systemen?\n",
      "   Kategorie: Vector Retrieval\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Systematischer Test mit allen Testfragen"
   ],
   "id": "c61e68905556485e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.428413Z",
     "start_time": "2025-08-10T17:52:40.221711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alle Testfragen durchlaufen\n",
    "print(\"=== SYSTEMATISCHER TEST ALLER TESTFRAGEN ===\")\n",
    "\n",
    "vector_results = []\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    query = row['question']\n",
    "    question_id = row['id']\n",
    "    difficulty = row['difficulty']\n",
    "\n",
    "    # Retrieval durchführen\n",
    "    results = vector_retrieval(query, top_k=3)\n",
    "\n",
    "    # Bestes Ergebnis speichern\n",
    "    if results:\n",
    "        best_result = results[0]\n",
    "        vector_results.append({\n",
    "            'question_id': question_id,\n",
    "            'query': query,\n",
    "            'difficulty': difficulty,\n",
    "            'retrieved_doc_id': best_result['document_id'],\n",
    "            'retrieval_score': best_result['score'],\n",
    "            'retrieved_question': best_result['question'],\n",
    "            'retrieved_category': best_result['category']\n",
    "        })\n",
    "\n",
    "    print(f\"{question_id}: {difficulty} - Score: {results[0]['score']:.3f}\")\n",
    "\n",
    "print(f\"\\n{len(vector_results)} Testfragen abgearbeitet\")"
   ],
   "id": "4e15e72321ba1109",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEMATISCHER TEST ALLER TESTFRAGEN ===\n",
      "q001: easy - Score: 0.272\n",
      "q002: medium - Score: 0.702\n",
      "q003: medium - Score: 0.701\n",
      "q004: hard - Score: 0.594\n",
      "q005: easy - Score: 0.688\n",
      "q006: hard - Score: 0.808\n",
      "q007: medium - Score: 0.764\n",
      "q008: medium - Score: 0.708\n",
      "q009: hard - Score: 0.641\n",
      "q010: medium - Score: 0.638\n",
      "q011: easy - Score: 0.697\n",
      "q012: hard - Score: 0.522\n",
      "\n",
      "12 Testfragen abgearbeitet\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse analysieren"
   ],
   "id": "7aaacdc42f82b246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.464439Z",
     "start_time": "2025-08-10T17:52:40.443656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ergebnisse in DataFrame konvertieren\n",
    "vector_df = pd.DataFrame(vector_results)\n",
    "\n",
    "print(\"=== VECTOR RETRIEVAL ANALYSE ===\")\n",
    "print(f\"Anzahl Testfragen: {len(vector_df)}\")\n",
    "print(f\"Durchschnittlicher Score: {vector_df['retrieval_score'].mean():.3f}\")\n",
    "print(f\"Minimaler Score: {vector_df['retrieval_score'].min():.3f}\")\n",
    "print(f\"Maximaler Score: {vector_df['retrieval_score'].max():.3f}\")\n",
    "\n",
    "# Analyse nach Schwierigkeit\n",
    "print(\"\\n=== SCORES NACH SCHWIERIGKEIT ===\")\n",
    "difficulty_stats = vector_df.groupby('difficulty')['retrieval_score'].agg(['count', 'mean', 'std']).round(3)\n",
    "print(difficulty_stats)\n",
    "\n",
    "# Top 5 und Bottom 5 Ergebnisse\n",
    "print(\"\\n=== TOP 5 RETRIEVAL ERGEBNISSE ===\")\n",
    "top_5 = vector_df.nlargest(5, 'retrieval_score')[['question_id', 'difficulty', 'retrieval_score', 'query']]\n",
    "print(top_5.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== SCHWÄCHSTE 5 RETRIEVAL ERGEBNISSE ===\")\n",
    "bottom_5 = vector_df.nsmallest(5, 'retrieval_score')[['question_id', 'difficulty', 'retrieval_score', 'query']]\n",
    "print(bottom_5.to_string(index=False))"
   ],
   "id": "e4163e68ada50c67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VECTOR RETRIEVAL ANALYSE ===\n",
      "Anzahl Testfragen: 12\n",
      "Durchschnittlicher Score: 0.645\n",
      "Minimaler Score: 0.272\n",
      "Maximaler Score: 0.808\n",
      "\n",
      "=== SCORES NACH SCHWIERIGKEIT ===\n",
      "            count   mean    std\n",
      "difficulty                     \n",
      "easy            3  0.552  0.243\n",
      "hard            4  0.641  0.121\n",
      "medium          5  0.703  0.045\n",
      "\n",
      "=== TOP 5 RETRIEVAL ERGEBNISSE ===\n",
      "question_id difficulty  retrieval_score                                                                query\n",
      "       q006       hard         0.807947                              Wie implementiere ich Hybrid Retrieval?\n",
      "       q007     medium         0.763958                            Welche Vector Database sollte ich wählen?\n",
      "       q008     medium         0.708346              Was sind häufige Probleme bei RAG und wie löse ich sie?\n",
      "       q002     medium         0.702070      Was sind die Unterschiede zwischen Vektor- und Graph-Retrieval?\n",
      "       q003     medium         0.701064 Welche Evaluationsmetriken sollte ich für mein RAG-System verwenden?\n",
      "\n",
      "=== SCHWÄCHSTE 5 RETRIEVAL ERGEBNISSE ===\n",
      "question_id difficulty  retrieval_score                                                 query\n",
      "       q001       easy         0.271852                   Erkläre mir RAG in einfachen Worten\n",
      "       q012       hard         0.521753          Wie verbessere ich meine Embedding-Qualität?\n",
      "       q004       hard         0.594258 Wie optimiere ich die Performance meines RAG-Systems?\n",
      "       q010     medium         0.638057  Was ist der optimale Chunk-Size für meine Dokumente?\n",
      "       q009       hard         0.640635            Wie funktioniert RAG mit Knowledge Graphs?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse speichern"
   ],
   "id": "a6c17e2ef82eb748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.502916Z",
     "start_time": "2025-08-10T17:52:40.490528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vector Retrieval Ergebnisse speichern\n",
    "vector_df.to_csv('../results/vector_retrieval_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# FAISS Index und Metadata speichern für spätere Nutzung\n",
    "faiss.write_index(index, '../data/faiss_index.bin')\n",
    "\n",
    "with open('../data/vector_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'document_metadata': document_metadata,\n",
    "        'model_name': model_name,\n",
    "        'embedding_dimension': dimension\n",
    "    }, f)\n",
    "\n",
    "print(\"Ergebnisse gespeichert:\")\n",
    "print(\"  - ../results/vector_retrieval_results.csv\")\n",
    "print(\"  - ../data/faiss_index.bin\")\n",
    "print(\"  - ../data/vector_metadata.pkl\")"
   ],
   "id": "cce35191e6c2909c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse gespeichert:\n",
      "  - ../results/vector_retrieval_results.csv\n",
      "  - ../data/faiss_index.bin\n",
      "  - ../data/vector_metadata.pkl\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:52:40.537036Z",
     "start_time": "2025-08-10T17:52:40.532721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== VECTOR RETRIEVAL ABGESCHLOSSEN ===\")\n",
    "print(\"FAISS-Index aufgebaut und getestet\")\n",
    "print(\"12 Testfragen erfolgreich abgearbeitet\")\n",
    "print(\"Ergebnisse gespeichert\")\n",
    "print(f\"Durchschnittlicher Retrieval-Score: {vector_df['retrieval_score'].mean():.3f}\")\n"
   ],
   "id": "231996f934b9a479",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VECTOR RETRIEVAL ABGESCHLOSSEN ===\n",
      "FAISS-Index aufgebaut und getestet\n",
      "12 Testfragen erfolgreich abgearbeitet\n",
      "Ergebnisse gespeichert\n",
      "Durchschnittlicher Retrieval-Score: 0.645\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
