{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04 - LLM Integration mit LangChain\n",
    "\n",
    "Integration der Retrieval-Methoden mit einem Large Language Model für vollständige RAG-Pipeline.\n",
    "\n",
    "## Ziele:\n",
    "- LangChain für RAG-Pipeline einrichten\n",
    "- Vector- und Graph-Retrieval mit LLM verbinden\n",
    "- Antworten generieren und vergleichen\n",
    "- RAG-Pipeline für Evaluation vorbereiten"
   ],
   "id": "518728e08983a933"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.145260Z",
     "start_time": "2025-08-10T21:22:42.141089Z"
    }
   },
   "source": [
    "# Imports\n",
    "import json\n",
    "from http.client import responses\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# LangChain Imports (korrigiert)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"Imports erfolgreich\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports erfolgreich\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI API Setup\n",
    "\n",
    "**Wichtig:** Es wird ein OpenAI API Key benötigt.\n",
    "- Der Key muss in einer Datei `openai_key.txt` im Root-Verzeichnis des Projekts gespeichert werden\n",
    "\n",
    "Falls kein OpenAI Key verfügbar ist, nutzen wir ein lokales Modell als Fallback."
   ],
   "id": "bec78ae68e943595"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.190463Z",
     "start_time": "2025-08-10T21:22:42.185748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OpenAI API Key Setup\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "# Versuche API Key aus Datei zu lesen\n",
    "try:\n",
    "    key_file_path = \"../openai_key.txt\"  # Pfad zur Key-Datei im Root\n",
    "    with open(key_file_path, 'r', encoding='utf-8') as f:\n",
    "        OPENAI_API_KEY = f.read().strip()\n",
    "    print(\"OpenAI API Key aus Datei geladen\")\n",
    "except FileNotFoundError:\n",
    "    print(\"openai_key.txt nicht gefunden im Root-Verzeichnis\")\n",
    "    # Fallback: Umgebungsvariable\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if OPENAI_API_KEY:\n",
    "        print(\"OpenAI API Key aus Umgebungsvariable geladen\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden des API Keys: {e}\")\n",
    "\n",
    "# API Key konfigurieren\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"OpenAI API Key konfiguriert\")\n",
    "    USE_OPENAI = True\n",
    "else:\n",
    "    print(\"Kein OpenAI API Key gefunden\")\n",
    "    print(\"Verwende lokales Modell als Fallback\")\n",
    "    print(\"Erstelle openai_key.txt im Root-Verzeichnis mit deinem API Key\")\n",
    "    USE_OPENAI = False"
   ],
   "id": "9e941f363a5426f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key aus Datei geladen\n",
      "OpenAI API Key konfiguriert\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Daten und vorherige Ergebnisse laden"
   ],
   "id": "6913e89c20087c3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.234397Z",
     "start_time": "2025-08-10T21:22:42.226703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FAQ-Korpus laden\n",
    "with open('../data/faq_korpus.json', 'r', encoding='utf-8') as f:\n",
    "    faq_documents = json.load(f)\n",
    "\n",
    "# Testfragen laden\n",
    "test_questions = pd.read_csv('../data/fragenliste.csv')\n",
    "\n",
    "# Vector Retrieval Ergebnisse laden\n",
    "vector_df = pd.read_csv('../results/vector_retrieval_results.csv')\n",
    "\n",
    "# Graph Retrieval Ergebnisse laden\n",
    "graph_df = pd.read_csv('../results/graph_retrieval_results.csv')\n",
    "\n",
    "print(f\"{len(faq_documents)} FAQ-Dokumente geladen\")\n",
    "print(f\"{len(test_questions)} Testfragen geladen\")\n",
    "print(f\"Vector Retrieval Ergebnisse geladen\")\n",
    "print(f\"Graph Retrieval Ergebnisse geladen\")"
   ],
   "id": "a17ea2d6f55586f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 FAQ-Dokumente geladen\n",
      "12 Testfragen geladen\n",
      "Vector Retrieval Ergebnisse geladen\n",
      "Graph Retrieval Ergebnisse geladen\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLM initialisieren"
   ],
   "id": "27361b874185420a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.274265Z",
     "start_time": "2025-08-10T21:22:42.269243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LLM initialisieren\n",
    "if USE_OPENAI:\n",
    "    # OpenAI ChatGPT\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        temperature=0.1,  # Niedrige Temperatur für konsistente Antworten\n",
    "        max_tokens=500\n",
    "    )\n",
    "    print(\"OpenAI ChatGPT-4.1-nano initialisiert\")\n",
    "else:\n",
    "    # Fallback: Dummy LLM für Demo\n",
    "    class DummyLLM:\n",
    "        def __call__(self, prompt):\n",
    "            return \"Demo-Antwort: Diese Antwort wurde ohne echtes LLM generiert, da kein OpenAI API Key verfügbar ist.\"\n",
    "\n",
    "    llm = DummyLLM()\n",
    "    print(\"Dummy-LLM initialisiert (Demo-Modus)\")"
   ],
   "id": "e42ae7a38ecc0025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI ChatGPT-4.1-nano initialisiert\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-Prompt Templates definieren"
   ],
   "id": "c9eeca18e8d28564"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.319389Z",
     "start_time": "2025-08-10T21:22:42.315488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prompt Template für RAG\n",
    "rag_prompt_template = \"\"\"Du bist ein hilfsreicher Assistent für Fragen zu Retrieval-Augmented Generation (RAG) und verwandten Technologien.\n",
    "\n",
    "Kontext-Informationen:\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Anweisungen:\n",
    "- Beantworte die Frage basierend auf den bereitgestellten Kontext-Informationen\n",
    "- Wenn die Informationen nicht ausreichen, sage das ehrlich\n",
    "- Halte deine Antwort präzise und hilfreich\n",
    "- Verwende die Fachbegriffe aus dem Kontext korrekt\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "# Prompt für Vergleich ohne Kontext (als Baseline)\n",
    "baseline_prompt_template = \"\"\"Du bist ein hilfsreicher Assistent für Fragen zu Retrieval-Augmented Generation (RAG) und verwandten Technologien.\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Beantworte die Frage basierend auf deinem allgemeinen Wissen über RAG, Machine Learning und AI.\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "print(\"Prompt Templates definiert\")"
   ],
   "id": "8df1720ca9b6e073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Templates definiert\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vector Retrieval RAG-Pipeline"
   ],
   "id": "f25fea5d61d01a5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:45.127725Z",
     "start_time": "2025-08-10T21:22:42.362204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vector Retrieval laden\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS Index und Metadaten laden\n",
    "faiss_index = faiss.read_index('../data/faiss_index.bin')\n",
    "with open('../data/vector_metadata.pkl', 'rb') as f:\n",
    "    vector_metadata = pickle.load(f)\n",
    "\n",
    "embedding_model = SentenceTransformer(vector_metadata['model_name'])\n",
    "\n",
    "def vector_rag_pipeline(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Vollständige Vector RAG Pipeline\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Query Embedding generieren\n",
    "    query_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "    # 2. Ähnliche Dokumente finden\n",
    "    scores, indices = faiss_index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    # 3. Kontext zusammenstellen\n",
    "    retrieved_docs = []\n",
    "    context_parts = []\n",
    "\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:\n",
    "            doc_meta = vector_metadata['document_metadata'][idx]\n",
    "            retrieved_docs.append(doc_meta)\n",
    "            context_parts.append(f\"Dokument {i+1}: {doc_meta['answer']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4. LLM-Antwort generieren (KORRIGIERT)\n",
    "    if USE_OPENAI:\n",
    "        prompt = rag_prompt_template.format(context=context, question=question)\n",
    "        # Korrigierter LLM-Aufruf für ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    else:\n",
    "        answer = f\"Demo-Antwort basierend auf {len(retrieved_docs)} Dokumenten: {question}\"\n",
    "\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'retrieval_scores': scores[0].tolist() if len(scores[0]) > 0 else [],\n",
    "        'retrieval_time': retrieval_time,\n",
    "        'method': 'vector'\n",
    "    }\n",
    "\n",
    "print(\"Vector RAG Pipeline implementiert\")"
   ],
   "id": "5941b96cb12f9e0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector RAG Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graph Retrieval RAG-Pipeline"
   ],
   "id": "ac49b132875df65b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:47.908163Z",
     "start_time": "2025-08-10T21:22:45.149794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph Retrieval RAG Pipeline\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "\n",
    "# Neo4j wieder verbinden\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    nlp = spacy.load(\"de_core_news_sm\") if spacy.util.is_package(\"de_core_news_sm\") else spacy.load(\"en_core_web_sm\")\n",
    "    GRAPH_AVAILABLE = True\n",
    "    print(\"Neo4j-Verbindung und Spacy geladen\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph Retrieval nicht verfügbar: {e}\")\n",
    "    GRAPH_AVAILABLE = False\n",
    "\n",
    "def extract_query_terms(text):\n",
    "    \"\"\"Extrahiert Suchbegriffe aus der Query\"\"\"\n",
    "    doc = nlp(text)\n",
    "    terms = []\n",
    "\n",
    "    # Named Entities\n",
    "    for ent in doc.ents:\n",
    "        if len(ent.text.strip()) > 2:\n",
    "            terms.append(ent.text.strip().lower())\n",
    "\n",
    "    # Wichtige Konzepte\n",
    "    for chunk in doc.noun_chunks:\n",
    "        term = chunk.text.strip().lower()\n",
    "        if len(term) > 3 and not term.startswith(('der', 'die', 'das')):\n",
    "            terms.append(term)\n",
    "\n",
    "    # Fallback: einfache Wörter\n",
    "    simple_terms = [word.lower().strip() for word in text.split()\n",
    "                   if len(word) > 3 and word.lower() not in ['sind', 'eine', 'wie', 'was']]\n",
    "\n",
    "    return list(set(terms + simple_terms))\n",
    "\n",
    "def graph_rag_pipeline(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Vollständige Graph RAG Pipeline\n",
    "    \"\"\"\n",
    "    if not GRAPH_AVAILABLE:\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': \"Graph Retrieval nicht verfügbar - Neo4j nicht verbunden\",\n",
    "            'retrieved_docs': [],\n",
    "            'retrieval_scores': [],\n",
    "            'retrieval_time': 0,\n",
    "            'method': 'graph'\n",
    "        }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Query-Begriffe extrahieren\n",
    "    query_terms = extract_query_terms(question)\n",
    "\n",
    "    if not query_terms:\n",
    "        query_terms = [question.lower()]\n",
    "\n",
    "    # 2. Graph-Suche durchführen\n",
    "    with driver.session() as session:\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH (d:Document)\n",
    "        OPTIONAL MATCH (d)-[:MENTIONS]->(c:Concept)\n",
    "        OPTIONAL MATCH (d)-[:CONTAINS]->(e:Entity)\n",
    "\n",
    "        WITH d,\n",
    "             collect(DISTINCT c.name) as concepts,\n",
    "             collect(DISTINCT e.name) as entities\n",
    "\n",
    "        WITH d, concepts, entities,\n",
    "             REDUCE(score = 0, term IN $query_terms |\n",
    "                 score +\n",
    "                 CASE WHEN term IN concepts THEN 3 ELSE 0 END +\n",
    "                 CASE WHEN term IN entities THEN 2 ELSE 0 END +\n",
    "                 CASE WHEN toLower(d.question) CONTAINS term THEN 2 ELSE 0 END +\n",
    "                 CASE WHEN toLower(d.answer) CONTAINS term THEN 1 ELSE 0 END\n",
    "             ) as relevance_score\n",
    "\n",
    "        WHERE relevance_score > 0\n",
    "\n",
    "        RETURN d.id as doc_id,\n",
    "               d.question as question,\n",
    "               d.answer as answer,\n",
    "               d.category as category,\n",
    "               relevance_score\n",
    "\n",
    "        ORDER BY relevance_score DESC\n",
    "        LIMIT $top_k\n",
    "        \"\"\"\n",
    "\n",
    "        result = session.run(cypher_query, {\n",
    "            'query_terms': query_terms,\n",
    "            'top_k': top_k\n",
    "        })\n",
    "\n",
    "        # 3. Kontext zusammenstellen\n",
    "        retrieved_docs = []\n",
    "        context_parts = []\n",
    "        scores = []\n",
    "\n",
    "        for i, record in enumerate(result):\n",
    "            doc_info = {\n",
    "                'id': record['doc_id'],\n",
    "                'question': record['question'],\n",
    "                'answer': record['answer'],\n",
    "                'category': record['category']\n",
    "            }\n",
    "            retrieved_docs.append(doc_info)\n",
    "            context_parts.append(f\"Dokument {i+1}: {doc_info['answer']}\")\n",
    "            scores.append(record['relevance_score'])\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4. LLM-Antwort generieren (KORRIGIERT)\n",
    "    if USE_OPENAI and context:\n",
    "        prompt = rag_prompt_template.format(context=context, question=question)\n",
    "        # Korrigierter LLM-Aufruf für ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    elif context:\n",
    "        answer = f\"Demo-Antwort basierend auf {len(retrieved_docs)} Graph-Dokumenten: {question}\"\n",
    "    else:\n",
    "        answer = \"Keine relevanten Dokumente im Knowledge Graph gefunden.\"\n",
    "\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'retrieval_scores': scores,\n",
    "        'retrieval_time': retrieval_time,\n",
    "        'method': 'graph'\n",
    "    }\n",
    "\n",
    "print(\"Graph RAG Pipeline implementiert\")"
   ],
   "id": "5392e51ca9e69069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j-Verbindung und Spacy geladen\n",
      "Graph RAG Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Baseline ohne Retrieval"
   ],
   "id": "8a16feccff599152"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:47.936782Z",
     "start_time": "2025-08-10T21:22:47.932293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def baseline_pipeline(question: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Baseline: LLM ohne Retrieval (nur allgemeines Wissen)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if USE_OPENAI:\n",
    "        prompt = baseline_prompt_template.format(question=question)\n",
    "        # Korrigierter LLM-Aufruf für ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    else:\n",
    "        answer = f\"Demo-Baseline-Antwort ohne Retrieval: {question}\"\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': [],\n",
    "        'retrieval_scores': [],\n",
    "        'retrieval_time': generation_time,\n",
    "        'method': 'baseline'\n",
    "    }\n",
    "\n",
    "print(\"Baseline Pipeline implementiert\")"
   ],
   "id": "9db9c95ed82e920a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test der RAG-Pipelines"
   ],
   "id": "fbd57a5991faa68f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:51.269662Z",
     "start_time": "2025-08-10T21:22:47.975657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test mit einer Beispielfrage\n",
    "test_question = \"Was ist RAG?\"\n",
    "\n",
    "print(\"=== RAG-PIPELINE TESTS ===\")\n",
    "print(f\"Testfrage: '{test_question}'\")\n",
    "\n",
    "# Vector RAG\n",
    "print(\"\\n--- VECTOR RAG ---\")\n",
    "vector_result = vector_rag_pipeline(test_question)\n",
    "print(f\"Retrieval Zeit: {vector_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Gefundene Dokumente: {len(vector_result['retrieved_docs'])}\")\n",
    "print(f\"Antwort: {vector_result['answer'][:200]}...\")\n",
    "\n",
    "# Graph RAG\n",
    "print(\"\\n--- GRAPH RAG ---\")\n",
    "graph_result = graph_rag_pipeline(test_question)\n",
    "print(f\"Retrieval Zeit: {graph_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Gefundene Dokumente: {len(graph_result['retrieved_docs'])}\")\n",
    "print(f\"Antwort: {graph_result['answer'][:200]}...\")\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n--- BASELINE ---\")\n",
    "baseline_result = baseline_pipeline(test_question)\n",
    "print(f\"Generation Zeit: {baseline_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Antwort: {baseline_result['answer'][:200]}...\")"
   ],
   "id": "7ec5c85980b96dff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG-PIPELINE TESTS ===\n",
      "Testfrage: 'Was ist RAG?'\n",
      "\n",
      "--- VECTOR RAG ---\n",
      "Retrieval Zeit: 1.059s\n",
      "Gefundene Dokumente: 3\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell kann relevante Informationen aus einer Wissensbasis abrufen und diese...\n",
      "\n",
      "--- GRAPH RAG ---\n",
      "Retrieval Zeit: 0.829s\n",
      "Gefundene Dokumente: 3\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell kann relevante Informationen aus einer Wissensbasis abrufen und diese...\n",
      "\n",
      "--- BASELINE ---\n",
      "Generation Zeit: 1.400s\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine fortschrittliche Methode im Bereich der Künstlichen Intelligenz, die die Stärken von Retrieval-Systemen und generativen Sprachmodellen kombiniert. Dabei w...\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:51.299952Z",
     "start_time": "2025-08-10T21:22:51.298250Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "9934cb417cf04a5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vollständige Evaluation aller Testfragen"
   ],
   "id": "b480a1537bbd7259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.002755Z",
     "start_time": "2025-08-10T21:22:51.330186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alle Testfragen durch alle Pipelines laufen lassen\n",
    "print(\"=== VOLLSTÄNDIGE RAG-PIPELINE EVALUATION ===\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['question']\n",
    "    question_id = row['id']\n",
    "    difficulty = row['difficulty']\n",
    "\n",
    "    print(f\"🔄 Verarbeite {question_id} ({difficulty}): {question[:50]}...\")\n",
    "\n",
    "    # Vector RAG\n",
    "    vector_result = vector_rag_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'vector',\n",
    "        'answer': vector_result['answer'],\n",
    "        'num_retrieved': len(vector_result['retrieved_docs']),\n",
    "        'retrieval_time': vector_result['retrieval_time'],\n",
    "        'avg_retrieval_score': np.mean(vector_result['retrieval_scores']) if vector_result['retrieval_scores'] else 0\n",
    "    })\n",
    "\n",
    "    # Graph RAG\n",
    "    graph_result = graph_rag_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'graph',\n",
    "        'answer': graph_result['answer'],\n",
    "        'num_retrieved': len(graph_result['retrieved_docs']),\n",
    "        'retrieval_time': graph_result['retrieval_time'],\n",
    "        'avg_retrieval_score': np.mean(graph_result['retrieval_scores']) if graph_result['retrieval_scores'] else 0\n",
    "    })\n",
    "\n",
    "    # Baseline\n",
    "    baseline_result = baseline_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'baseline',\n",
    "        'answer': baseline_result['answer'],\n",
    "        'num_retrieved': 0,\n",
    "        'retrieval_time': baseline_result['retrieval_time'],\n",
    "        'avg_retrieval_score': 0\n",
    "    })\n",
    "\n",
    "print(f\"{len(all_results)} Antworten generiert ({len(all_results)//3} Fragen × 3 Methoden)\")"
   ],
   "id": "4525ad6e1d06f14f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOLLSTÄNDIGE RAG-PIPELINE EVALUATION ===\n",
      "Verarbeite q001 (easy): Erkläre mir RAG in einfachen Worten...\n",
      "Verarbeite q002 (medium): Was sind die Unterschiede zwischen Vektor- und Gra...\n",
      "Verarbeite q003 (medium): Welche Evaluationsmetriken sollte ich für mein RAG...\n",
      "Verarbeite q004 (hard): Wie optimiere ich die Performance meines RAG-Syste...\n",
      "Verarbeite q005 (easy): Was kostet der Betrieb eines RAG-Systems?...\n",
      "Verarbeite q006 (hard): Wie implementiere ich Hybrid Retrieval?...\n",
      "Verarbeite q007 (medium): Welche Vector Database sollte ich wählen?...\n",
      "Verarbeite q008 (medium): Was sind häufige Probleme bei RAG und wie löse ich...\n",
      "Verarbeite q009 (hard): Wie funktioniert RAG mit Knowledge Graphs?...\n",
      "Verarbeite q010 (medium): Was ist der optimale Chunk-Size für meine Dokument...\n",
      "Verarbeite q011 (easy): Welche Zukunftstrends gibt es bei RAG?...\n",
      "Verarbeite q012 (hard): Wie verbessere ich meine Embedding-Qualität?...\n",
      "36 Antworten generiert (12 Fragen × 3 Methoden)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse analysieren"
   ],
   "id": "7baf0039c579761f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.036290Z",
     "start_time": "2025-08-10T21:23:50.023081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ergebnisse in DataFrame konvertieren\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=== RAG-PIPELINE ANALYSE ===\")\n",
    "print(f\"Gesamte Antworten: {len(results_df)}\")\n",
    "\n",
    "# Analyse nach Methode\n",
    "method_stats = results_df.groupby('method').agg({\n",
    "    'retrieval_time': ['mean', 'std'],\n",
    "    'num_retrieved': 'mean',\n",
    "    'avg_retrieval_score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE NACH METHODE ===\")\n",
    "print(method_stats)\n",
    "\n",
    "# Analyse nach Schwierigkeit\n",
    "difficulty_stats = results_df.groupby(['difficulty', 'method'])['retrieval_time'].mean().unstack().round(3)\n",
    "print(\"\\n=== RETRIEVAL-ZEIT NACH SCHWIERIGKEIT UND METHODE ===\")\n",
    "print(difficulty_stats)\n",
    "\n",
    "# Erfolgsrate (Dokumente gefunden)\n",
    "success_rate = results_df[results_df['method'] != 'baseline'].groupby('method')['num_retrieved'].apply(lambda x: (x > 0).mean()).round(3)\n",
    "print(\"\\n=== ERFOLGSRATE (Dokumente gefunden) ===\")\n",
    "print(success_rate)"
   ],
   "id": "a3d4d847d9944087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG-PIPELINE ANALYSE ===\n",
      "Gesamte Antworten: 36\n",
      "\n",
      "=== PERFORMANCE NACH METHODE ===\n",
      "         retrieval_time        num_retrieved avg_retrieval_score\n",
      "                   mean    std          mean                mean\n",
      "method                                                          \n",
      "baseline          3.007  1.080         0.000               0.000\n",
      "graph             0.764  0.597         1.917               3.417\n",
      "vector            1.117  0.333         3.000               0.520\n",
      "\n",
      "=== RETRIEVAL-ZEIT NACH SCHWIERIGKEIT UND METHODE ===\n",
      "method      baseline  graph  vector\n",
      "difficulty                         \n",
      "easy           2.473  0.853   1.030\n",
      "hard           3.247  0.607   1.201\n",
      "medium         3.137  0.837   1.101\n",
      "\n",
      "=== ERFOLGSRATE (Dokumente gefunden) ===\n",
      "method\n",
      "graph     0.667\n",
      "vector    1.000\n",
      "Name: num_retrieved, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Beispiel-Antworten vergleichen"
   ],
   "id": "1c719d89391c520c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.078819Z",
     "start_time": "2025-08-10T21:23:50.073273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Beispiel-Vergleich für eine mittelschwere Frage\n",
    "sample_question_id = 'q002'  # \"Was sind die Unterschiede zwischen Vektor- und Graph-Retrieval?\"\n",
    "\n",
    "sample_results = results_df[results_df['question_id'] == sample_question_id]\n",
    "\n",
    "print(\"=== ANTWORTEN-VERGLEICH ===\")\n",
    "print(f\"Frage: {sample_results.iloc[0]['question']}\")\n",
    "\n",
    "for _, result in sample_results.iterrows():\n",
    "    print(f\"\\n--- {result['method'].upper()} ---\")\n",
    "    print(f\"Zeit: {result['retrieval_time']:.3f}s\")\n",
    "    print(f\"Gefundene Docs: {result['num_retrieved']}\")\n",
    "    print(f\"Antwort: {result['answer'][:300]}...\")"
   ],
   "id": "e446c9bb78bb0dea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANTWORTEN-VERGLEICH ===\n",
      "Frage: Was sind die Unterschiede zwischen Vektor- und Graph-Retrieval?\n",
      "\n",
      "--- VECTOR ---\n",
      "Zeit: 1.265s\n",
      "Gefundene Docs: 3\n",
      "Antwort: Vektor-Retrieval basiert auf der Umwandlung von Dokumenten und Anfragen in hochdimensionale Embeddings, wobei die Ähnlichkeit durch Metriken wie Cosinus-Ähnlichkeit gemessen wird, um semantische Beziehungen zu erfassen. Graph-Retrieval hingegen nutzt Graphstrukturen, um Beziehungen zwischen Entitäte...\n",
      "\n",
      "--- GRAPH ---\n",
      "Zeit: 1.562s\n",
      "Gefundene Docs: 3\n",
      "Antwort: Vektor-Retrieval nutzt neuronale Embeddings, um semantische Ähnlichkeiten zwischen Anfragen und Dokumenten zu erkennen, und basiert auf Dense Retrieval, das für semantische Suche geeignet ist. Graph-Retrieval hingegen erfasst Beziehungen zwischen Entitäten durch Graphstrukturen, ermöglicht komplexes...\n",
      "\n",
      "--- BASELINE ---\n",
      "Zeit: 2.200s\n",
      "Gefundene Docs: 0\n",
      "Antwort: Der Unterschied zwischen Vektor- und Graph-Retrieval liegt in der Art und Weise, wie Informationen gesucht und organisiert werden:\n",
      "\n",
      "1. Vektor-Retrieval:\n",
      "- Prinzip: Nutzt hochdimensionale Vektorrepräsentationen (Embeddings) von Texten oder Dokumenten.\n",
      "- Funktionsweise: Bei einer Suchanfrage werden Em...\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse speichern"
   ],
   "id": "c4bcff365c711249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.118893Z",
     "start_time": "2025-08-10T21:23:50.109651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG-Pipeline Ergebnisse speichern\n",
    "results_df.to_csv('../results/rag_pipeline_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Detaillierte Antworten für spätere Analyse speichern\n",
    "detailed_results = {}\n",
    "for idx, row in results_df.iterrows():\n",
    "    key = f\"{row['question_id']}_{row['method']}\"\n",
    "    detailed_results[key] = {\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'method': row['method'],\n",
    "        'difficulty': row['difficulty'],\n",
    "        'retrieval_time': row['retrieval_time']\n",
    "    }\n",
    "\n",
    "with open('../results/detailed_answers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"RAG-Pipeline Ergebnisse gespeichert:\")\n",
    "print(\"  - ../results/rag_pipeline_results.csv\")\n",
    "print(\"  - ../results/detailed_answers.json\")"
   ],
   "id": "b543c8fb79c42db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Pipeline Ergebnisse gespeichert:\n",
      "  - ../results/rag_pipeline_results.csv\n",
      "  - ../results/detailed_answers.json\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.150857Z",
     "start_time": "2025-08-10T21:23:50.146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== LLM INTEGRATION ABGESCHLOSSEN ===\")\n",
    "print(\"RAG-Pipelines für Vector, Graph und Baseline implementiert\")\n",
    "print(f\"{len(results_df)} Antworten generiert\")\n",
    "print(\"Detaillierte Ergebnisse gespeichert\")\n",
    "print(f\"Durchschnittliche Antwortzeit: {results_df['retrieval_time'].mean():.3f}s\")\n",
    "\n",
    "# Aufräumen\n",
    "if GRAPH_AVAILABLE:\n",
    "    driver.close()\n",
    "    print(\"Neo4j-Verbindung geschlossen\")"
   ],
   "id": "978ba0314f37ab34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM INTEGRATION ABGESCHLOSSEN ===\n",
      "RAG-Pipelines für Vector, Graph und Baseline implementiert\n",
      "36 Antworten generiert\n",
      "Detaillierte Ergebnisse gespeichert\n",
      "Durchschnittliche Antwortzeit: 1.629s\n",
      "\n",
      "Neo4j-Verbindung geschlossen\n"
     ]
    }
   ],
   "execution_count": 74
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
