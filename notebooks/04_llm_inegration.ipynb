{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04 - LLM Integration mit LangChain\n",
    "\n",
    "Integration der Retrieval-Methoden mit einem Large Language Model f√ºr vollst√§ndige RAG-Pipeline.\n",
    "\n",
    "## Ziele:\n",
    "- LangChain f√ºr RAG-Pipeline einrichten\n",
    "- Vector- und Graph-Retrieval mit LLM verbinden\n",
    "- Antworten generieren und vergleichen\n",
    "- RAG-Pipeline f√ºr Evaluation vorbereiten"
   ],
   "id": "518728e08983a933"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.145260Z",
     "start_time": "2025-08-10T21:22:42.141089Z"
    }
   },
   "source": [
    "# Imports\n",
    "import json\n",
    "from http.client import responses\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# LangChain Imports (korrigiert)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"Imports erfolgreich\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports erfolgreich\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI API Setup\n",
    "\n",
    "**Wichtig:** Es wird ein OpenAI API Key ben√∂tigt.\n",
    "- Der Key muss in einer Datei `openai_key.txt` im Root-Verzeichnis des Projekts gespeichert werden\n",
    "\n",
    "Falls kein OpenAI Key verf√ºgbar ist, nutzen wir ein lokales Modell als Fallback."
   ],
   "id": "bec78ae68e943595"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.190463Z",
     "start_time": "2025-08-10T21:22:42.185748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OpenAI API Key Setup\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "# Versuche API Key aus Datei zu lesen\n",
    "try:\n",
    "    key_file_path = \"../openai_key.txt\"  # Pfad zur Key-Datei im Root\n",
    "    with open(key_file_path, 'r', encoding='utf-8') as f:\n",
    "        OPENAI_API_KEY = f.read().strip()\n",
    "    print(\"OpenAI API Key aus Datei geladen\")\n",
    "except FileNotFoundError:\n",
    "    print(\"openai_key.txt nicht gefunden im Root-Verzeichnis\")\n",
    "    # Fallback: Umgebungsvariable\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if OPENAI_API_KEY:\n",
    "        print(\"OpenAI API Key aus Umgebungsvariable geladen\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden des API Keys: {e}\")\n",
    "\n",
    "# API Key konfigurieren\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(\"OpenAI API Key konfiguriert\")\n",
    "    USE_OPENAI = True\n",
    "else:\n",
    "    print(\"Kein OpenAI API Key gefunden\")\n",
    "    print(\"Verwende lokales Modell als Fallback\")\n",
    "    print(\"Erstelle openai_key.txt im Root-Verzeichnis mit deinem API Key\")\n",
    "    USE_OPENAI = False"
   ],
   "id": "9e941f363a5426f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key aus Datei geladen\n",
      "OpenAI API Key konfiguriert\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Daten und vorherige Ergebnisse laden"
   ],
   "id": "6913e89c20087c3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.234397Z",
     "start_time": "2025-08-10T21:22:42.226703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FAQ-Korpus laden\n",
    "with open('../data/faq_korpus.json', 'r', encoding='utf-8') as f:\n",
    "    faq_documents = json.load(f)\n",
    "\n",
    "# Testfragen laden\n",
    "test_questions = pd.read_csv('../data/fragenliste.csv')\n",
    "\n",
    "# Vector Retrieval Ergebnisse laden\n",
    "vector_df = pd.read_csv('../results/vector_retrieval_results.csv')\n",
    "\n",
    "# Graph Retrieval Ergebnisse laden\n",
    "graph_df = pd.read_csv('../results/graph_retrieval_results.csv')\n",
    "\n",
    "print(f\"{len(faq_documents)} FAQ-Dokumente geladen\")\n",
    "print(f\"{len(test_questions)} Testfragen geladen\")\n",
    "print(f\"Vector Retrieval Ergebnisse geladen\")\n",
    "print(f\"Graph Retrieval Ergebnisse geladen\")"
   ],
   "id": "a17ea2d6f55586f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 FAQ-Dokumente geladen\n",
      "12 Testfragen geladen\n",
      "Vector Retrieval Ergebnisse geladen\n",
      "Graph Retrieval Ergebnisse geladen\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLM initialisieren"
   ],
   "id": "27361b874185420a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.274265Z",
     "start_time": "2025-08-10T21:22:42.269243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LLM initialisieren\n",
    "if USE_OPENAI:\n",
    "    # OpenAI ChatGPT\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        temperature=0.1,  # Niedrige Temperatur f√ºr konsistente Antworten\n",
    "        max_tokens=500\n",
    "    )\n",
    "    print(\"OpenAI ChatGPT-4.1-nano initialisiert\")\n",
    "else:\n",
    "    # Fallback: Dummy LLM f√ºr Demo\n",
    "    class DummyLLM:\n",
    "        def __call__(self, prompt):\n",
    "            return \"Demo-Antwort: Diese Antwort wurde ohne echtes LLM generiert, da kein OpenAI API Key verf√ºgbar ist.\"\n",
    "\n",
    "    llm = DummyLLM()\n",
    "    print(\"Dummy-LLM initialisiert (Demo-Modus)\")"
   ],
   "id": "e42ae7a38ecc0025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI ChatGPT-4.1-nano initialisiert\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-Prompt Templates definieren"
   ],
   "id": "c9eeca18e8d28564"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:42.319389Z",
     "start_time": "2025-08-10T21:22:42.315488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prompt Template f√ºr RAG\n",
    "rag_prompt_template = \"\"\"Du bist ein hilfsreicher Assistent f√ºr Fragen zu Retrieval-Augmented Generation (RAG) und verwandten Technologien.\n",
    "\n",
    "Kontext-Informationen:\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Anweisungen:\n",
    "- Beantworte die Frage basierend auf den bereitgestellten Kontext-Informationen\n",
    "- Wenn die Informationen nicht ausreichen, sage das ehrlich\n",
    "- Halte deine Antwort pr√§zise und hilfreich\n",
    "- Verwende die Fachbegriffe aus dem Kontext korrekt\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "# Prompt f√ºr Vergleich ohne Kontext (als Baseline)\n",
    "baseline_prompt_template = \"\"\"Du bist ein hilfsreicher Assistent f√ºr Fragen zu Retrieval-Augmented Generation (RAG) und verwandten Technologien.\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Beantworte die Frage basierend auf deinem allgemeinen Wissen √ºber RAG, Machine Learning und AI.\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "print(\"Prompt Templates definiert\")"
   ],
   "id": "8df1720ca9b6e073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Templates definiert\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vector Retrieval RAG-Pipeline"
   ],
   "id": "f25fea5d61d01a5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:45.127725Z",
     "start_time": "2025-08-10T21:22:42.362204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vector Retrieval laden\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS Index und Metadaten laden\n",
    "faiss_index = faiss.read_index('../data/faiss_index.bin')\n",
    "with open('../data/vector_metadata.pkl', 'rb') as f:\n",
    "    vector_metadata = pickle.load(f)\n",
    "\n",
    "embedding_model = SentenceTransformer(vector_metadata['model_name'])\n",
    "\n",
    "def vector_rag_pipeline(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Vollst√§ndige Vector RAG Pipeline\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Query Embedding generieren\n",
    "    query_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "    # 2. √Ñhnliche Dokumente finden\n",
    "    scores, indices = faiss_index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    # 3. Kontext zusammenstellen\n",
    "    retrieved_docs = []\n",
    "    context_parts = []\n",
    "\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:\n",
    "            doc_meta = vector_metadata['document_metadata'][idx]\n",
    "            retrieved_docs.append(doc_meta)\n",
    "            context_parts.append(f\"Dokument {i+1}: {doc_meta['answer']}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4. LLM-Antwort generieren (KORRIGIERT)\n",
    "    if USE_OPENAI:\n",
    "        prompt = rag_prompt_template.format(context=context, question=question)\n",
    "        # Korrigierter LLM-Aufruf f√ºr ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    else:\n",
    "        answer = f\"Demo-Antwort basierend auf {len(retrieved_docs)} Dokumenten: {question}\"\n",
    "\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'retrieval_scores': scores[0].tolist() if len(scores[0]) > 0 else [],\n",
    "        'retrieval_time': retrieval_time,\n",
    "        'method': 'vector'\n",
    "    }\n",
    "\n",
    "print(\"Vector RAG Pipeline implementiert\")"
   ],
   "id": "5941b96cb12f9e0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector RAG Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graph Retrieval RAG-Pipeline"
   ],
   "id": "ac49b132875df65b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:47.908163Z",
     "start_time": "2025-08-10T21:22:45.149794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Graph Retrieval RAG Pipeline\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "\n",
    "# Neo4j wieder verbinden\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    nlp = spacy.load(\"de_core_news_sm\") if spacy.util.is_package(\"de_core_news_sm\") else spacy.load(\"en_core_web_sm\")\n",
    "    GRAPH_AVAILABLE = True\n",
    "    print(\"Neo4j-Verbindung und Spacy geladen\")\n",
    "except Exception as e:\n",
    "    print(f\"Graph Retrieval nicht verf√ºgbar: {e}\")\n",
    "    GRAPH_AVAILABLE = False\n",
    "\n",
    "def extract_query_terms(text):\n",
    "    \"\"\"Extrahiert Suchbegriffe aus der Query\"\"\"\n",
    "    doc = nlp(text)\n",
    "    terms = []\n",
    "\n",
    "    # Named Entities\n",
    "    for ent in doc.ents:\n",
    "        if len(ent.text.strip()) > 2:\n",
    "            terms.append(ent.text.strip().lower())\n",
    "\n",
    "    # Wichtige Konzepte\n",
    "    for chunk in doc.noun_chunks:\n",
    "        term = chunk.text.strip().lower()\n",
    "        if len(term) > 3 and not term.startswith(('der', 'die', 'das')):\n",
    "            terms.append(term)\n",
    "\n",
    "    # Fallback: einfache W√∂rter\n",
    "    simple_terms = [word.lower().strip() for word in text.split()\n",
    "                   if len(word) > 3 and word.lower() not in ['sind', 'eine', 'wie', 'was']]\n",
    "\n",
    "    return list(set(terms + simple_terms))\n",
    "\n",
    "def graph_rag_pipeline(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Vollst√§ndige Graph RAG Pipeline\n",
    "    \"\"\"\n",
    "    if not GRAPH_AVAILABLE:\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': \"Graph Retrieval nicht verf√ºgbar - Neo4j nicht verbunden\",\n",
    "            'retrieved_docs': [],\n",
    "            'retrieval_scores': [],\n",
    "            'retrieval_time': 0,\n",
    "            'method': 'graph'\n",
    "        }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Query-Begriffe extrahieren\n",
    "    query_terms = extract_query_terms(question)\n",
    "\n",
    "    if not query_terms:\n",
    "        query_terms = [question.lower()]\n",
    "\n",
    "    # 2. Graph-Suche durchf√ºhren\n",
    "    with driver.session() as session:\n",
    "        cypher_query = \"\"\"\n",
    "        MATCH (d:Document)\n",
    "        OPTIONAL MATCH (d)-[:MENTIONS]->(c:Concept)\n",
    "        OPTIONAL MATCH (d)-[:CONTAINS]->(e:Entity)\n",
    "\n",
    "        WITH d,\n",
    "             collect(DISTINCT c.name) as concepts,\n",
    "             collect(DISTINCT e.name) as entities\n",
    "\n",
    "        WITH d, concepts, entities,\n",
    "             REDUCE(score = 0, term IN $query_terms |\n",
    "                 score +\n",
    "                 CASE WHEN term IN concepts THEN 3 ELSE 0 END +\n",
    "                 CASE WHEN term IN entities THEN 2 ELSE 0 END +\n",
    "                 CASE WHEN toLower(d.question) CONTAINS term THEN 2 ELSE 0 END +\n",
    "                 CASE WHEN toLower(d.answer) CONTAINS term THEN 1 ELSE 0 END\n",
    "             ) as relevance_score\n",
    "\n",
    "        WHERE relevance_score > 0\n",
    "\n",
    "        RETURN d.id as doc_id,\n",
    "               d.question as question,\n",
    "               d.answer as answer,\n",
    "               d.category as category,\n",
    "               relevance_score\n",
    "\n",
    "        ORDER BY relevance_score DESC\n",
    "        LIMIT $top_k\n",
    "        \"\"\"\n",
    "\n",
    "        result = session.run(cypher_query, {\n",
    "            'query_terms': query_terms,\n",
    "            'top_k': top_k\n",
    "        })\n",
    "\n",
    "        # 3. Kontext zusammenstellen\n",
    "        retrieved_docs = []\n",
    "        context_parts = []\n",
    "        scores = []\n",
    "\n",
    "        for i, record in enumerate(result):\n",
    "            doc_info = {\n",
    "                'id': record['doc_id'],\n",
    "                'question': record['question'],\n",
    "                'answer': record['answer'],\n",
    "                'category': record['category']\n",
    "            }\n",
    "            retrieved_docs.append(doc_info)\n",
    "            context_parts.append(f\"Dokument {i+1}: {doc_info['answer']}\")\n",
    "            scores.append(record['relevance_score'])\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4. LLM-Antwort generieren (KORRIGIERT)\n",
    "    if USE_OPENAI and context:\n",
    "        prompt = rag_prompt_template.format(context=context, question=question)\n",
    "        # Korrigierter LLM-Aufruf f√ºr ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    elif context:\n",
    "        answer = f\"Demo-Antwort basierend auf {len(retrieved_docs)} Graph-Dokumenten: {question}\"\n",
    "    else:\n",
    "        answer = \"Keine relevanten Dokumente im Knowledge Graph gefunden.\"\n",
    "\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'retrieval_scores': scores,\n",
    "        'retrieval_time': retrieval_time,\n",
    "        'method': 'graph'\n",
    "    }\n",
    "\n",
    "print(\"Graph RAG Pipeline implementiert\")"
   ],
   "id": "5392e51ca9e69069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j-Verbindung und Spacy geladen\n",
      "Graph RAG Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Baseline ohne Retrieval"
   ],
   "id": "8a16feccff599152"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:47.936782Z",
     "start_time": "2025-08-10T21:22:47.932293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def baseline_pipeline(question: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Baseline: LLM ohne Retrieval (nur allgemeines Wissen)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if USE_OPENAI:\n",
    "        prompt = baseline_prompt_template.format(question=question)\n",
    "        # Korrigierter LLM-Aufruf f√ºr ChatOpenAI\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm.invoke(messages)\n",
    "        answer = response.content\n",
    "    else:\n",
    "        answer = f\"Demo-Baseline-Antwort ohne Retrieval: {question}\"\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'retrieved_docs': [],\n",
    "        'retrieval_scores': [],\n",
    "        'retrieval_time': generation_time,\n",
    "        'method': 'baseline'\n",
    "    }\n",
    "\n",
    "print(\"Baseline Pipeline implementiert\")"
   ],
   "id": "9db9c95ed82e920a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Pipeline implementiert\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test der RAG-Pipelines"
   ],
   "id": "fbd57a5991faa68f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:51.269662Z",
     "start_time": "2025-08-10T21:22:47.975657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test mit einer Beispielfrage\n",
    "test_question = \"Was ist RAG?\"\n",
    "\n",
    "print(\"=== RAG-PIPELINE TESTS ===\")\n",
    "print(f\"Testfrage: '{test_question}'\")\n",
    "\n",
    "# Vector RAG\n",
    "print(\"\\n--- VECTOR RAG ---\")\n",
    "vector_result = vector_rag_pipeline(test_question)\n",
    "print(f\"Retrieval Zeit: {vector_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Gefundene Dokumente: {len(vector_result['retrieved_docs'])}\")\n",
    "print(f\"Antwort: {vector_result['answer'][:200]}...\")\n",
    "\n",
    "# Graph RAG\n",
    "print(\"\\n--- GRAPH RAG ---\")\n",
    "graph_result = graph_rag_pipeline(test_question)\n",
    "print(f\"Retrieval Zeit: {graph_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Gefundene Dokumente: {len(graph_result['retrieved_docs'])}\")\n",
    "print(f\"Antwort: {graph_result['answer'][:200]}...\")\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n--- BASELINE ---\")\n",
    "baseline_result = baseline_pipeline(test_question)\n",
    "print(f\"Generation Zeit: {baseline_result['retrieval_time']:.3f}s\")\n",
    "print(f\"Antwort: {baseline_result['answer'][:200]}...\")"
   ],
   "id": "7ec5c85980b96dff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG-PIPELINE TESTS ===\n",
      "Testfrage: 'Was ist RAG?'\n",
      "\n",
      "--- VECTOR RAG ---\n",
      "Retrieval Zeit: 1.059s\n",
      "Gefundene Dokumente: 3\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell kann relevante Informationen aus einer Wissensbasis abrufen und diese...\n",
      "\n",
      "--- GRAPH RAG ---\n",
      "Retrieval Zeit: 0.829s\n",
      "Gefundene Dokumente: 3\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine Technik, die Large Language Models mit externen Wissensquellen verbindet. Das Modell kann relevante Informationen aus einer Wissensbasis abrufen und diese...\n",
      "\n",
      "--- BASELINE ---\n",
      "Generation Zeit: 1.400s\n",
      "Antwort: RAG (Retrieval-Augmented Generation) ist eine fortschrittliche Methode im Bereich der K√ºnstlichen Intelligenz, die die St√§rken von Retrieval-Systemen und generativen Sprachmodellen kombiniert. Dabei w...\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:22:51.299952Z",
     "start_time": "2025-08-10T21:22:51.298250Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "9934cb417cf04a5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vollst√§ndige Evaluation aller Testfragen"
   ],
   "id": "b480a1537bbd7259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.002755Z",
     "start_time": "2025-08-10T21:22:51.330186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alle Testfragen durch alle Pipelines laufen lassen\n",
    "print(\"=== VOLLST√ÑNDIGE RAG-PIPELINE EVALUATION ===\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in test_questions.iterrows():\n",
    "    question = row['question']\n",
    "    question_id = row['id']\n",
    "    difficulty = row['difficulty']\n",
    "\n",
    "    print(f\"üîÑ Verarbeite {question_id} ({difficulty}): {question[:50]}...\")\n",
    "\n",
    "    # Vector RAG\n",
    "    vector_result = vector_rag_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'vector',\n",
    "        'answer': vector_result['answer'],\n",
    "        'num_retrieved': len(vector_result['retrieved_docs']),\n",
    "        'retrieval_time': vector_result['retrieval_time'],\n",
    "        'avg_retrieval_score': np.mean(vector_result['retrieval_scores']) if vector_result['retrieval_scores'] else 0\n",
    "    })\n",
    "\n",
    "    # Graph RAG\n",
    "    graph_result = graph_rag_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'graph',\n",
    "        'answer': graph_result['answer'],\n",
    "        'num_retrieved': len(graph_result['retrieved_docs']),\n",
    "        'retrieval_time': graph_result['retrieval_time'],\n",
    "        'avg_retrieval_score': np.mean(graph_result['retrieval_scores']) if graph_result['retrieval_scores'] else 0\n",
    "    })\n",
    "\n",
    "    # Baseline\n",
    "    baseline_result = baseline_pipeline(question)\n",
    "    all_results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'method': 'baseline',\n",
    "        'answer': baseline_result['answer'],\n",
    "        'num_retrieved': 0,\n",
    "        'retrieval_time': baseline_result['retrieval_time'],\n",
    "        'avg_retrieval_score': 0\n",
    "    })\n",
    "\n",
    "print(f\"{len(all_results)} Antworten generiert ({len(all_results)//3} Fragen √ó 3 Methoden)\")"
   ],
   "id": "4525ad6e1d06f14f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOLLST√ÑNDIGE RAG-PIPELINE EVALUATION ===\n",
      "Verarbeite q001 (easy): Erkl√§re mir RAG in einfachen Worten...\n",
      "Verarbeite q002 (medium): Was sind die Unterschiede zwischen Vektor- und Gra...\n",
      "Verarbeite q003 (medium): Welche Evaluationsmetriken sollte ich f√ºr mein RAG...\n",
      "Verarbeite q004 (hard): Wie optimiere ich die Performance meines RAG-Syste...\n",
      "Verarbeite q005 (easy): Was kostet der Betrieb eines RAG-Systems?...\n",
      "Verarbeite q006 (hard): Wie implementiere ich Hybrid Retrieval?...\n",
      "Verarbeite q007 (medium): Welche Vector Database sollte ich w√§hlen?...\n",
      "Verarbeite q008 (medium): Was sind h√§ufige Probleme bei RAG und wie l√∂se ich...\n",
      "Verarbeite q009 (hard): Wie funktioniert RAG mit Knowledge Graphs?...\n",
      "Verarbeite q010 (medium): Was ist der optimale Chunk-Size f√ºr meine Dokument...\n",
      "Verarbeite q011 (easy): Welche Zukunftstrends gibt es bei RAG?...\n",
      "Verarbeite q012 (hard): Wie verbessere ich meine Embedding-Qualit√§t?...\n",
      "36 Antworten generiert (12 Fragen √ó 3 Methoden)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse analysieren"
   ],
   "id": "7baf0039c579761f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.036290Z",
     "start_time": "2025-08-10T21:23:50.023081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ergebnisse in DataFrame konvertieren\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=== RAG-PIPELINE ANALYSE ===\")\n",
    "print(f\"Gesamte Antworten: {len(results_df)}\")\n",
    "\n",
    "# Analyse nach Methode\n",
    "method_stats = results_df.groupby('method').agg({\n",
    "    'retrieval_time': ['mean', 'std'],\n",
    "    'num_retrieved': 'mean',\n",
    "    'avg_retrieval_score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE NACH METHODE ===\")\n",
    "print(method_stats)\n",
    "\n",
    "# Analyse nach Schwierigkeit\n",
    "difficulty_stats = results_df.groupby(['difficulty', 'method'])['retrieval_time'].mean().unstack().round(3)\n",
    "print(\"\\n=== RETRIEVAL-ZEIT NACH SCHWIERIGKEIT UND METHODE ===\")\n",
    "print(difficulty_stats)\n",
    "\n",
    "# Erfolgsrate (Dokumente gefunden)\n",
    "success_rate = results_df[results_df['method'] != 'baseline'].groupby('method')['num_retrieved'].apply(lambda x: (x > 0).mean()).round(3)\n",
    "print(\"\\n=== ERFOLGSRATE (Dokumente gefunden) ===\")\n",
    "print(success_rate)"
   ],
   "id": "a3d4d847d9944087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG-PIPELINE ANALYSE ===\n",
      "Gesamte Antworten: 36\n",
      "\n",
      "=== PERFORMANCE NACH METHODE ===\n",
      "         retrieval_time        num_retrieved avg_retrieval_score\n",
      "                   mean    std          mean                mean\n",
      "method                                                          \n",
      "baseline          3.007  1.080         0.000               0.000\n",
      "graph             0.764  0.597         1.917               3.417\n",
      "vector            1.117  0.333         3.000               0.520\n",
      "\n",
      "=== RETRIEVAL-ZEIT NACH SCHWIERIGKEIT UND METHODE ===\n",
      "method      baseline  graph  vector\n",
      "difficulty                         \n",
      "easy           2.473  0.853   1.030\n",
      "hard           3.247  0.607   1.201\n",
      "medium         3.137  0.837   1.101\n",
      "\n",
      "=== ERFOLGSRATE (Dokumente gefunden) ===\n",
      "method\n",
      "graph     0.667\n",
      "vector    1.000\n",
      "Name: num_retrieved, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Beispiel-Antworten vergleichen"
   ],
   "id": "1c719d89391c520c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.078819Z",
     "start_time": "2025-08-10T21:23:50.073273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Beispiel-Vergleich f√ºr eine mittelschwere Frage\n",
    "sample_question_id = 'q002'  # \"Was sind die Unterschiede zwischen Vektor- und Graph-Retrieval?\"\n",
    "\n",
    "sample_results = results_df[results_df['question_id'] == sample_question_id]\n",
    "\n",
    "print(\"=== ANTWORTEN-VERGLEICH ===\")\n",
    "print(f\"Frage: {sample_results.iloc[0]['question']}\")\n",
    "\n",
    "for _, result in sample_results.iterrows():\n",
    "    print(f\"\\n--- {result['method'].upper()} ---\")\n",
    "    print(f\"Zeit: {result['retrieval_time']:.3f}s\")\n",
    "    print(f\"Gefundene Docs: {result['num_retrieved']}\")\n",
    "    print(f\"Antwort: {result['answer'][:300]}...\")"
   ],
   "id": "e446c9bb78bb0dea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANTWORTEN-VERGLEICH ===\n",
      "Frage: Was sind die Unterschiede zwischen Vektor- und Graph-Retrieval?\n",
      "\n",
      "--- VECTOR ---\n",
      "Zeit: 1.265s\n",
      "Gefundene Docs: 3\n",
      "Antwort: Vektor-Retrieval basiert auf der Umwandlung von Dokumenten und Anfragen in hochdimensionale Embeddings, wobei die √Ñhnlichkeit durch Metriken wie Cosinus-√Ñhnlichkeit gemessen wird, um semantische Beziehungen zu erfassen. Graph-Retrieval hingegen nutzt Graphstrukturen, um Beziehungen zwischen Entit√§te...\n",
      "\n",
      "--- GRAPH ---\n",
      "Zeit: 1.562s\n",
      "Gefundene Docs: 3\n",
      "Antwort: Vektor-Retrieval nutzt neuronale Embeddings, um semantische √Ñhnlichkeiten zwischen Anfragen und Dokumenten zu erkennen, und basiert auf Dense Retrieval, das f√ºr semantische Suche geeignet ist. Graph-Retrieval hingegen erfasst Beziehungen zwischen Entit√§ten durch Graphstrukturen, erm√∂glicht komplexes...\n",
      "\n",
      "--- BASELINE ---\n",
      "Zeit: 2.200s\n",
      "Gefundene Docs: 0\n",
      "Antwort: Der Unterschied zwischen Vektor- und Graph-Retrieval liegt in der Art und Weise, wie Informationen gesucht und organisiert werden:\n",
      "\n",
      "1. Vektor-Retrieval:\n",
      "- Prinzip: Nutzt hochdimensionale Vektorrepr√§sentationen (Embeddings) von Texten oder Dokumenten.\n",
      "- Funktionsweise: Bei einer Suchanfrage werden Em...\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ergebnisse speichern"
   ],
   "id": "c4bcff365c711249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.118893Z",
     "start_time": "2025-08-10T21:23:50.109651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG-Pipeline Ergebnisse speichern\n",
    "results_df.to_csv('../results/rag_pipeline_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Detaillierte Antworten f√ºr sp√§tere Analyse speichern\n",
    "detailed_results = {}\n",
    "for idx, row in results_df.iterrows():\n",
    "    key = f\"{row['question_id']}_{row['method']}\"\n",
    "    detailed_results[key] = {\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'method': row['method'],\n",
    "        'difficulty': row['difficulty'],\n",
    "        'retrieval_time': row['retrieval_time']\n",
    "    }\n",
    "\n",
    "with open('../results/detailed_answers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"RAG-Pipeline Ergebnisse gespeichert:\")\n",
    "print(\"  - ../results/rag_pipeline_results.csv\")\n",
    "print(\"  - ../results/detailed_answers.json\")"
   ],
   "id": "b543c8fb79c42db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Pipeline Ergebnisse gespeichert:\n",
      "  - ../results/rag_pipeline_results.csv\n",
      "  - ../results/detailed_answers.json\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:50.150857Z",
     "start_time": "2025-08-10T21:23:50.146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== LLM INTEGRATION ABGESCHLOSSEN ===\")\n",
    "print(\"RAG-Pipelines f√ºr Vector, Graph und Baseline implementiert\")\n",
    "print(f\"{len(results_df)} Antworten generiert\")\n",
    "print(\"Detaillierte Ergebnisse gespeichert\")\n",
    "print(f\"Durchschnittliche Antwortzeit: {results_df['retrieval_time'].mean():.3f}s\")\n",
    "\n",
    "# Aufr√§umen\n",
    "if GRAPH_AVAILABLE:\n",
    "    driver.close()\n",
    "    print(\"Neo4j-Verbindung geschlossen\")"
   ],
   "id": "978ba0314f37ab34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM INTEGRATION ABGESCHLOSSEN ===\n",
      "RAG-Pipelines f√ºr Vector, Graph und Baseline implementiert\n",
      "36 Antworten generiert\n",
      "Detaillierte Ergebnisse gespeichert\n",
      "Durchschnittliche Antwortzeit: 1.629s\n",
      "\n",
      "Neo4j-Verbindung geschlossen\n"
     ]
    }
   ],
   "execution_count": 74
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
